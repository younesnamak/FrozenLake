{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I8QQya0DMqr_"
      },
      "outputs": [],
      "source": [
        "!pip install gymnasium"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Sequential"
      ],
      "metadata": {
        "id": "dlfZ0j53MvI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('FrozenLake-v1', desc=None, map_name=\"4x4\", is_slippery=False, render_mode='rgb_array')\n",
        "env.reset()\n",
        "env.render()"
      ],
      "metadata": {
        "id": "m0AKPM12NeUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test this code\n",
        "# number_of_par = env.observation_space.n\n",
        "# # if type(number_of_par) == list:\n",
        "# #   number_of_par = len(number_of_par)\n",
        "# # else:\n",
        "# #   number_of_par = 1\n",
        "\n",
        "# number_of_par = len(number_of_par) if type(number_of_par) == list else 1\n",
        "# number_of_par"
      ],
      "metadata": {
        "id": "YBvbdAVdQBur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DqnAgent:\n",
        "\n",
        "  def __init__(self, env):\n",
        "    self.env=env\n",
        "    self.memory=deque(maxlen=2000)\n",
        "    self.gamma=0.3\n",
        "    self.epsilon=1.0\n",
        "    self.epsilon_min=0.01\n",
        "    self.epsilon_decay=0.995\n",
        "    self.learning_rate=0.005\n",
        "    self.tau=0.125\n",
        "    self.model=self.create_model()\n",
        "    self.target_model=self.create_model()\n",
        "\n",
        "  def action(self, state):\n",
        "    self.epsilon *= self.epsilon_decay\n",
        "    self.epsilon = max(self.epsilon , self.epsilon_min)\n",
        "\n",
        "    if np.random.random()<self.epsilon:\n",
        "      return self.env.action_space.sample()\n",
        "    else:\n",
        "      return np.argmax(self.model.predict(state))\n",
        "\n",
        "  def create_model(self):\n",
        "    number_of_par = self.env.observation_space.n\n",
        "    number_of_par = len(number_of_par) if type(number_of_par) == list else 1\n",
        "    model=Sequential()\n",
        "    model.add(Dense(24,input_dim=number_of_par,activation=\"relu\"))\n",
        "    model.add(Dense(48,activation=\"relu\"))\n",
        "    model.add(Dense(24,activation=\"relu\"))\n",
        "    model.add(Dense(self.env.action_space.n))\n",
        "    model.compile(loss=\"mean_squared_error\",optimizer=Adam(learning_rate=self.learning_rate))\n",
        "    return model\n",
        "\n",
        "  def remembr(self,state,action,reward,new_state,done):\n",
        "    self.memory.append([state,action,reward,new_state,done])\n",
        "\n",
        "  def target_train(self):\n",
        "    weights=self.model.get_weights()\n",
        "    target_weights=self.target_model.get_weights()\n",
        "\n",
        "    for i in range(len(target_weights)):\n",
        "      target_weights[i]= weights[i] * self.tau + target_weights[i] * (1-self.tau)\n",
        "\n",
        "    self.target_model.set_weights(target_weights)\n",
        "\n",
        "  def save_model(self,path):\n",
        "    self.model.save(path)\n",
        "\n",
        "  def reply(self):\n",
        "    batch_size = 32\n",
        "    if len(self.memory) < batch_size:\n",
        "        return\n",
        "\n",
        "    samples = random.sample(self.memory, batch_size)\n",
        "    for sample in samples:\n",
        "        state, action, reward, new_state, done = sample\n",
        "\n",
        "        # Reshape state and new_state to match the model input shape\n",
        "        state = np.array(state).reshape(1, -1)  # Ensure it is (1, input_dim)\n",
        "        new_state = np.array(new_state).reshape(1, -1)\n",
        "\n",
        "        # Get the target predictions\n",
        "        target = self.target_model.predict(state)\n",
        "\n",
        "        if done:\n",
        "            target[0][action] = reward\n",
        "        else:\n",
        "            Q_future = max(self.target_model.predict(new_state)[0])\n",
        "            target[0][action] = reward + self.gamma * Q_future\n",
        "\n",
        "        # Fit the model (reshape state and target as required)\n",
        "        self.model.fit(state, target, epochs=1, verbose=1)\n"
      ],
      "metadata": {
        "id": "5fhgRlwrOIwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trails=500\n",
        "trail_len=500\n",
        "dqn_agent=DqnAgent(env)"
      ],
      "metadata": {
        "id": "RSHyhYIsYyYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output"
      ],
      "metadata": {
        "id": "FFGZwwRZZwsH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for trail in range(trails):\n",
        "  current_state=np.array([[env.reset()[0]]])\n",
        "  for step in range(trail_len):\n",
        "    print(\"#\",step)\n",
        "    action=dqn_agent.action(current_state)\n",
        "    new_state,reward,done,_,_=env.step(action)\n",
        "    new_state=np.array([[new_state]])\n",
        "    dqn_agent.remembr(current_state,action,reward,new_state,done)\n",
        "    dqn_agent.reply()\n",
        "    dqn_agent.target_train()\n",
        "    current_state=new_state\n",
        "    clear_output(wait=True)\n",
        "    if done:\n",
        "      break\n",
        "  if step>199:\n",
        "    print(\"failed\")\n",
        "  else:\n",
        "    print(\"success\")\n",
        "    dqn_agent.save_model('result.h5')\n",
        "    break"
      ],
      "metadata": {
        "id": "6xAQcGXxY_Ch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "done = False\n",
        "state = np.array([[env.reset()[0]]])\n",
        "frames=[]\n",
        "epoches=0\n",
        "\n",
        "while not done:\n",
        "  predict_action = dqn_agent.target_model.predict(state)\n",
        "  action = np.argmax(predict_action[0])\n",
        "  new_state,reward,done,_,_=env.step(action)\n",
        "  print(done)\n",
        "  state =  np.array([[new_state]])\n",
        "  frames.append({'frame':env.render(),\n",
        "                 'state':state,\n",
        "                 'action':action,\n",
        "                 'reward':reward})\n",
        "  epoches+=1\n",
        "  print(f\"epoche : {epoches}\")\n",
        "  if epoches == 199:\n",
        "    break"
      ],
      "metadata": {
        "id": "GMVj2pKpad8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i ,frame in enumerate(frames):\n",
        "    plt.title(f\"Frame : {i}\")\n",
        "    plt.imshow(frame['frame'])\n",
        "    plt.pause(0.2)\n",
        "    plt.clf()"
      ],
      "metadata": {
        "id": "Bd_hS4v7hGw9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}